{
    "collab_server" : "",
    "contents" : "######################################################################\n### setup the environment\n######################################################################\nscriptName = \"targeting.r\";\nprojectPath = \"/mnt/data/targeting/\"   # note spelling error\nprojectCode = paste0(projectPath, \"code/\");\n## care with what is stored locally!!!  onlu anonimized data can be stored locally\nprojectDataSet = paste0(projectPath, \"datasets/\")\nprojectResults = paste0(projectPath, \"results/\")\nprojectLibraries = paste0(projectPath, \"libraries/\")\n\n## if projectCode is not setup, just set to current working directory\nif ( !exists(\"projectCode\") ) {\n    projectPath = getwd();\n}\n\n# Set working directory to where data is located\nsetwd(projectCode);\nmessage( sprintf(\"Running %s from path: %s\", scriptName, getwd() ) );\n\n## MITRE has a proxy serving as a man in the micdle snooping the traffic\n## we need to point to this proxy server to load any global packages\nhttpNeeded = FALSE\nproxy = \"\"\nif (httpNeeded) {\n  proxy = \"http://gatekeeper.mitre.org:80\"\n}\nSys.setenv(http_proxy=proxy)\nSys.setenv(https_proxy=proxy)\nSys.getenv(\"http_proxy\")\nSys.getenv(\"https_proxy\")\n\n######################################################################\n## common defines...\n######################################################################\ncranMirror = \"https://cran.cnr.berkeley.edu/\"\nloadSourceEvaluation = FALSE   ## load library to evaluate a source file\n# from the data dictionary\ncolClasses = c( \"integer\",         # StrikeNumber\n                \"factor\",          # Classification\n                \"character\",       # Country\n                \"character\",       # DTG\n                \"character\",       # date\n                \"character\",       # time\n                \"character\",       # ATO\n                \"character\",       # EngageUnit\n                \"factor\",          # Del/Dyn\n                \"character\",       # EntityID\n                \"factor\",          # O-suffix\n                \"numeric\",         # CatCode\n                \"character\",       # EntityName\n                \"character\",       # City\n                \"character\",      # Province\n                \"character\",       # ROE\n                \"character\",       # TargetFunction\n                \"integer\",         # TF#1\n                \"character\",       # Target_TF#1\n                \"numeric\",         # lat\n                \"numeric\",         # long\n                \"character\",       # MGRS\n                \"character\",       # Observer\n                \"character\",       # Aircraft\n                \"character\",       # callsign\n                \"character\",       # ACBase\n                \"character\",       # nationality\n                \"integer\",         # ORD#1\n                \"character\",       # Ordinance_ORD#1\n                \"character\",       # target comments\n                \"character\",       # recoupe\n                \"character\",       # physical\n                \"character\",       # confidence\n                \"character\",       # functional\n                \"character\",       # MEA\n                \"character\",       # CDA\n                \"character\")       # MISREP\n\n######################################################################\n# Load the requisite libraries\n######################################################################\n##  source local project libraries first\nif(loadSourceEvaluation)\n    ## internal debugging helper, generally not used\n    # call via evaluteSourceExpressions(\"my-file.R\")\n    source(paste0(projectLibraries,\"EvaluateSourceExpressions.R\"))\n\n# load our postgres pdmp helper routines\n## load up required library loader routines (in projectLibraries)\nif(!exists(\"UsePackage\", mode=\"function\")) \n    source(paste0(projectLibraries,\"LoadLib.R\"))\n\n## source global packages second\n## required libraries (initially growing list....)\nlibraries <- c(\"ggplot2\", \n               \"tidyverse\",     # works well with stringr and dplyr\n               \"stringr\",       # nice string package\n               \"quanteda\",      # document matrix package\n               \"purrr\",         # \n               \"Rcpp\",\n               \"dplyr\",         # nice package to work with \"table\" data\n               \"pryr\",          # pryer helps to search namespace (conflicts happen)\n               \"data.table\",    # has a nice %like% option, largely overlaps with dplyr\n               \"RCurl\",         # a little more power for reading web content\n               \"lubridate\",     # date package\n               \"readr\",         # another csv reader package\n               \"pastecs\")       # love stat.desc - to get descriptive stats on a data frame\nfor(library in libraries) \n{ \n    if(!UsePackage(library, defaultCRANmirror = \"https://cran.cnr.berkeley.edu/\"))\n    {\n        # try manually to get error\n        install.packages(library, defaultCRANmirror = \"https://cran.cnr.berkeley.edu/\", dependencies =  T)\n        stop(\"LoadLibrary Error!\", library)\n    }\n}\n\n######################################################################\n# local support routines\n######################################################################\nmissingValues <- function(df) {\n    # are there any missing data\n    bMissing = any(is.na(df))\n    lMissing = list()\n    index = 1\n    if(bMissing) {\n        for (Var in names(df)) {\n            missing <- sum(is.na(df[,Var]))\n            if (missing > 0) {\n                print(c(Var,missing))\n                lMissing[[index]] = c(Var, missing)\n                index = index + 1\n            }\n        }\n    }\n    str(lMissing)\n    return(lMissing)\n}\ncategoricalAnalysis <- function(cats.df) {\n    str(cats.df)\n    names(cats.df)\n    lcat = list()\n    i = 1\n    for (col in names(cats.df)) {\n        col\n        ftab = table(cats.df[,col], useNA=\"ifany\")\n        ptab = prop.table(ftab)\n        lcat[[i]] = c(col, ftab, ptab)\n        i = i + 1\n    }\n    return(lcat)\n}\n# I like this approach\nprintf <- function(...)print(sprintf(...))\nprintDF <- function(df) df %>% head\n\nloadData <- function (dataFile) {\n    \n    if (missing(dataFile))\n        return(NULL)\n    \n    # Read data in from file, comma deliminated, header is true\n    raw.df = read.csv(file=dataFile, \n                      header=T, \n                      sep=',', \n                      strip.white = T,\n                      colClasses = colClasses,\n                      na.strings=c(\"\", \"NA\", \"?\", \" \"))\n    return(raw.df)\n}\n\n# features is a comma seperated list to query\n# featureSet = c(\"StrikeID\", \"T1\", \"Target_T1\")\nloadFeatures <- function(df, features) {\n    \n    if(missing(df) | missing(features))\n        return(NULL)\n    raw.df = df %>% select(features)\n    return(raw.df)\n}\n\n## see - http://www.mjdenny.com/Text_Processing_In_R.html\n## see - https://github.com/matthewjdenny\n## still tweaking\nClean_String <- function(string){\n    # Lowercase\n    temp <- tolower(string)\n    \n    #' Remove everything that is not a number or letter (may want to keep more \n    #' stuff in your actual analyses). \n    temp <- stringr::str_replace_all(temp,\"[^a-zA-Z\\\\s]\", \" \")\n    \n    # Shrink down to just one white space\n    temp <- stringr::str_replace_all(temp,\"[\\\\s]+\", \" \")\n    \n    # Split it\n    temp <- stringr::str_split(temp, \" \")[[1]]\n    \n    # Get rid of trailing \"\" if necessary\n    indexes <- which(temp == \"\")\n    if(length(indexes) > 0){\n        temp <- temp[-indexes]\n    } \n    return(temp)\n}\n#' function to clean text\n#' typical calling sequence:\n#       con <- file(\"Obama_Speech_2-24-09.txt\", \"r\", blocking = FALSE)\n#       text <- readLines(con)\n#       close(con) \nClean_Text_Block <- function(text){\n    if(length(text) <= 1){\n        # Check to see if there is any text at all with another conditional\n        if(length(text) == 0){\n            cat(\"There was no text in this document! \\n\")\n            to_return <- list(num_tokens = 0, unique_tokens = 0, text = \"\")\n        }\n        else {\n            # If there is , and only only one line of text then tokenize it\n            clean_text <- Clean_String(text)\n            num_tok <- length(clean_text)\n            num_uniq <- length(unique(clean_text))\n            to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)\n        }\n    }\n    else {\n        # Get rid of blank lines\n        indexes <- which(text == \"\")\n        if(length(indexes) > 0){\n            text <- text[-indexes]\n        }  \n        # Loop through the lines in the text and use the append() function to \n        clean_text <- Clean_String(text[1])\n        for(i in 2:length(text)){\n            # add them to a vector \n            clean_text <- append(clean_text,Clean_String(text[i]))\n        }\n        # Calculate the number of tokens and unique tokens and return them in a \n        # named list object.\n        num_tok <- length(clean_text)\n        num_uniq <- length(unique(clean_text))\n        to_return <- list(num_tokens = num_tok, unique_tokens = num_uniq, text = clean_text)\n    }\n    return(to_return)\n}\nparseMetricTokens <- function(field)\n{\n    # works very well...\n    split.pos <- gregexpr('\\\\(.*?)', field)[[1]]\n    split.length <- attr(split.pos, \"match.length\")\n    split.start <- sort(c(1, split.pos, split.pos+split.length))\n    split.end <- c(split.start[-1]-1, nchar(field))\n    split.results = substring(field,split.start,split.end)\n    split.results\n    index = 1;\n    tokenList = list()\n    # not quite: all <- unlist(strsplit(x, \"\\\\s+\"))\n    # not quite: y = unlist(strsplit(x, ')'))\n    # closer: regmatches(x, gregexpr(\"(?=\\\\().*?(?<=\\\\))\", x, perl=T))[[1]]\n    pat <- \"(?<=\\\\()([^()]*)(?=\\\\))\"\n    for (i in split.results) {\n        if (nchar(i) > 1) {\n            element = i\n            pos <- gregexpr('\\\\(.*?)', element)[[1]]\n            if (pos > 0)\n                element = regmatches(element, gregexpr(pat, element, perl=TRUE))\n            element2 = str_trim(element)\n            #printf(\"element: %s, element2: %s, length: %d\", element, element2, nchar(element))\n            tokenList[index] = element2\n            index = index + 1\n        }\n    }\n    printf(\"index: %d\", index)\n    if(index == 2)\n        # if there  is no numeric value (x), assume 1.\n        tokenList[index] = \"1\"\n    tokenList\n    df = as.data.frame(tokenList)\n    names(df) = c(\"level\", \"count\")\n    return(df)\n}\n\n# DOES NOT WORK - NOT Supported DPLYR\n#  need to investyigate purr or base::aggregate\n# sets - critical notes\n#   1) order is important...\n#       a) StrikeID MUST BE FIRST (this is the FK back to target table)\n#       b) METRIC must be second - this is because we reference columns by index\n#       c) example: meaSet = c(\"StrikeID\", \"MEA\")\n#   2) the dataframe to query against\nparseMetric <- function(metricSet, df)\n{\n    # no attrbute specification, no deal.\n    if(missing(metricSet) | missing(df))\n        return(NULL) \n    \n    # easy change\n    groupBy = 1\n    metric = 2\n    \n    # ok, have at it\n    metric.df = na.omit(loadFeatures(df, metricSet))\n\n    # parse out the single line, multiple token input\n    metric2.df = metric.df %>% rowwise() %>% mutate(level=parseMetricTokens(.[[2]])$level)\n    metric3.df = metric2.df %>% rowwise() %>% mutate(count=parseMetricTokens(.[[2]])$count)\n    metric4.df = metric3.df %>% mutate_at(vars(matches(\"count\")),funs(as.numeric))\n    rm(metric.df, metric2.df, metric3.df)\n    return(metric3.df)\n}\n# Constants to our mainline\n#\nrawDataFile = \"Targeting Data Analytics v4a.csv\";\ndataFile = paste0( projectDataSet, rawDataFile ); \n\n######################################################################\n# Begin\n######################################################################\n# load the data (web or local)\nraw.df = loadData(dataFile)\ndim(raw.df)\nstr(raw.df)\nnames(raw.df)\nhead(raw.df)\n\n# have a useable start to importing the data\\\n# 1) missing data... (actually looks pretty good)\nmissing = missingValues(raw.df)\nmissing\n\n# 2) work to correct improperly parsed feature types\n# might be easier to turn off strings as factors, then change the factor features?\nraw.df$DTG <- str_replace_all(as.character(raw.df$DTG), \"Z\", \"\")\nraw.df$DTG <- parse_date_time(raw.df$DTG, orders=\"dHMSby\")\nraw.df$Date <- paste(day(raw.df$DTG), month(raw.df$DTG, label=T), year(raw.df$DTG), sep=\"-\")\nraw.df$Time <- paste(hour(raw.df$DTG), minute(raw.df$DTG), sep = \":\")\nhead(raw.df)\n\n# have a loom at the categorical variable\ncats_pos = c(2, 9)\ncats.df = raw.df[, cats_pos]\nlcat = categoricalAnalysis(cats.df)\nlcat\n\n#numerical analysis - summary statistics (none)\n#stat.desc(raw.df)\n\n# some elements are repeating, split them off into there own list \n# (reference strike id).  Focus areas:\n# 1) critical elements (detailed target)\ntargetDetailSet = c(\"StrikeID\", \"T1\", \"Target_T1\")\ntd.df = na.omit(loadFeatures(raw.df, targetDetailSet))\n\n# get total tagets by strikeid\ntd.df %>% group_by(StrikeID, Target_T1) %>% summarise(totTargets = sum(T1)) %>% head\n\n# 2) Aircraft\naircraftSet = c(\"StrikeID\", \"Aircraft\", \"Callsign\", \"ACBase\", \"Nationality\")\nac.df = na.omit(loadFeatures(raw.df, aircraftSet))\nac.df %>% group_by(StrikeID, Aircraft) %>% head\nac.df %>% group_by(StrikeID) %>% summarize(AC = paste(Aircraft, collapse=\",\"), total=n()) %>% head\n\n# 3) Ordinance\nordSet = c(\"StrikeID\", \"ORD1\", \"Ordinance_1\")\nord.df = na.omit(loadFeatures(raw.df, ordSet))\nord.df %>% group_by(StrikeID) %>% head\nord.df %>% group_by(StrikeID) %>% summarize(Ordinance = paste(Ordinance_1, collapse=\",\")) %>% head\nord.df %>% group_by(StrikeID) %>% summarize(totalOrd=sum(ORD1), Ordinance = paste(Ordinance_1, collapse=\",\")) %>% head\n\n# 4) Physical, Confidence, Functional\n# Physical\n# these are a tad bit trickier..  \nSet = c(\"StrikeID\", \"Physical\")\ndf = na.omit(loadFeatures(raw.df, Set))\n## see https://stackoverflow.com/questions/26003574/r-dplyr-mutate-use-dynamic-variable-names\nvarLev <- paste(Set[2], \"level\", sep=\".\")\nvarCnt <- paste(Set[2], \"count\", sep=\".\")\n\n#immutable calls, therefore record intermediates and delete upon finaly assembly\n# parseMetricTokens gets the col value (passed a litteral)\ndf.2 = df %>% rowwise() %>% mutate(!!varLev := parseMetricTokens(Physical)$level)\ndf.3 = df.2 %>% rowwise() %>% mutate(!!varCnt := parseMetricTokens(Physical)$count)\nphy.df = df.3 %>% mutate_at(vars(matches(\"count\")),funs(as.numeric))\n#phy.df %>% group_by(StrikeID) %>% head\nrm(df, df.2, df.3)\nprintDF(phy.df)\n\n# have a loom at the categorical variable\nCats = c(3)\nCats.df = phy.df[, Cats]\nphyCatsTab = categoricalAnalysis(Cats.df)\nphyCatsTab\nrm(Cats.df)\n\n# possible spelling matches on destroyed - there are a lot of inconsistencies.\n# eventually will need a better mechanism in parseMetricTokens (see strsplit with regex?)\n##############################\n## broken...  need a better way\n# phy.df %>% group_by(StrikeID) %>% filter_(grepl(destroyedRegEx, vars(matches(\"level\"))))\n# phy.df %>% group_by(StrikeID) %>% filter_(grepl(destroyedRegEx, [,3]))\n# for now, hardcode ass required\ndestroyedRegExKeyWords = c(\"des\", \"troy\")\ndestroyedRegEx = paste(destroyedRegExKeyWords, collapse = \"|\")\nphy.df %>% group_by(StrikeID) %>% filter(grepl(destroyedRegEx, Physical.level)) %>% summarize(total=sum(Physical.count)) %>% head\nphy.df %>% group_by(StrikeID) %>% filter(str_detect(Physical.level, destroyedRegEx)) %>% summarize(total=sum(Physical.count)) %>% head\n\n# Confidence\nSet = c(\"StrikeID\", \"Confidence\")\ndf = na.omit(loadFeatures(raw.df, Set))\n## see https://stackoverflow.com/questions/26003574/r-dplyr-mutate-use-dynamic-variable-names\nvarLev <- paste(Set[2], \"level\", sep=\".\")\nvarCnt <- paste(Set[2], \"count\", sep=\".\")\n\n# parse out the single line, multiple token input\ndf.2 = df %>% rowwise() %>% mutate(!!varLev := parseMetricTokens(Confidence)$level)\ndf.3 = df.2 %>% rowwise() %>% mutate(!!varCnt := parseMetricTokens(Confidence)$count)\nconf.df = df.3 %>% mutate_at(vars(matches(\"count\")),funs(as.numeric))\nrm(df, df.2, df.3)\nprintDF(conf.df)\n\n# have a loom at the categorical variable\nCats.df = conf.df[, Cats]\nconfCatsTab = categoricalAnalysis(Cats.df)\nrm(Cats.df)\n\n# Functional - join of physical and confidence...\n# just display for now...  will require a new parse algorithm\nfunSet = c(\"StrikeID\", \"Functional\")\nfun.df = na.omit(loadFeatures(raw.df, funSet))\n\n# 5) MEA, CDA\n# MEA\n# sets - critical notes\n#   1) order is important...\n#       a) StrikeID MUST BE FIRST (this is the FK back to target table)\n#       b) METRIC must be second - this is because we reference columns by index\nSet = c(\"StrikeID\", \"MEA\")\ndf = na.omit(loadFeatures(raw.df, Set))\n\n## see https://stackoverflow.com/questions/26003574/r-dplyr-mutate-use-dynamic-variable-names\nvarLev <- paste(Set[2], \"level\", sep=\".\")\nvarCnt <- paste(Set[2], \"count\", sep=\".\")\n\n# parse out the single line, multiple token input\n# note parameter to parseMetricTokens is the column name,  When called this is actually the value\ndf.2 = df %>% rowwise() %>% mutate(!!varLev := parseMetricTokens(MEA)$level)\ndf.3 = df.2 %>% rowwise() %>% mutate(!!varCnt := parseMetricTokens(MEA)$count)\nmea.df = df.3 %>% mutate_at(vars(matches(\"count\")),funs(as.numeric))\nrm(df, df.2, df.3)\nprintDF(mea.df)\n\n#### info - trying to use dplky by  col index (to parameteriz) is not working\n####  mutate thinks varname is a literal variable name (not a variable)... \n### tricky to build named variable\n## using purrr to get some stats\n#mea4.df %>% split(.[[3]]) %>% map_dbl(function(x) sum(x[ ,4], na.rm = TRUE))\n##  using aggregate (does not work with column index)\n#aggregate(mea4.df[ ,4] ~ level, data = mea4.df, sum)\n# does work by name\n#aggregate(count ~ level, data = mea4.df, sum)\n\n# CDA\nSet = c(\"StrikeID\", \"CDA\")\ndf = na.omit(loadFeatures(raw.df, Set))\n\n## see https://stackoverflow.com/questions/26003574/r-dplyr-mutate-use-dynamic-variable-names\nvarLev <- paste(Set[2], \"level\", sep=\".\")\nvarCnt <- paste(Set[2], \"count\", sep=\".\")\n\n# parse out the single line, multiple token input\n# note parameter to parseMetricTokens is the column name,  When called this is actually the value\ndf.2 = df %>% rowwise() %>% mutate(!!varLev := parseMetricTokens(CDA)$level)\ndf.3 = df.2 %>% rowwise() %>% mutate(!!varCnt := parseMetricTokens(CDA)$count)\ncda.df = df.3 %>% mutate_at(vars(matches(\"count\")),funs(as.numeric))\nrm(df, df.2, df.3)\nprintDF(cda.df)\n\n#### join  metric for global metric package\ndfList = list(phy.df, conf.df, mea.df, cda.df)\nanalysis.df = left_join(phy.df, conf.df, by=\"StrikeID\") %>%\n              left_join(., mea.df, by=\"StrikeID\") %>%\n                left_join(., cda.df, by=\"StrikeID\")\nstr(analysis.df)\nprintDF(analysis.df)\n# keep a local copy just in case the internet is down\nwrite.csv(analysis.df, file = paste0(projectDataSet,\"analysis.csv\"))\n\n\n",
    "created" : 1503583812355.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1860512556",
    "id" : "1572108",
    "lastKnownWriteTime" : 1503078882,
    "last_content_update" : 1503078882,
    "path" : "/mnt/data/targeting/code/targeting v8.R",
    "project_path" : "targeting v8.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}